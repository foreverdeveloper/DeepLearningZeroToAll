'''
cost 함수와 gradient descent 알고리듬에 대해서 공부.

Linear Regression에서 배운 hypothesis와 이번에 배운 hypothesis를 비교해서 보여주고 있다.
hypothesis는 cost 함수를 구성하는 핵심이기 때문에, 여기서는 cost 함수 또한 이전과 달라져야 한다고 얘기하고 있다.
그림의 왼쪽 부분은 매끈한 밥그릇이고, 오른쪽 부분은 울퉁불퉁하다.
김성훈 교수님은 이 부분에 대해서 왼쪽은 직선을 살짝 구부려서 연결을 한 모양이고,
오른쪽은 sigmoid를 구부려서 연결을 했기 때문이라고 설명하셨다.

sigmoid 모양에 대해서 추가 설명을 한다면, e의 지수 형태의 그래프를 사용했기 때문에 구부러진 곡선이 연결된 느낌이다.
그러나, 실제로는 log 함수를 사용하기 때문에 매끄러운 밥그릇이 만들어진다.

오른쪽 그림에 교수님께서 Local이라고 쓰셨는데 이 부분을 매끄럽게 만들지 않으면
local minimum을 최저점으로 잘못 발견할 수 있는 문제가 발생한다.
그래서, 반드시 ★구불구불한 형태의 밥그릇을 매끈하게 펴야 한다. 그래야 global minimum에 도착할 수 있다.★


새로운 cost 함수다. cost 함수의 목적은 비용(cost)을 판단해서 올바른 W와 b를 찾는 것이다.
다시 말해, 목표로 하는 W와 b를 찾을 수 있다면, 어떤 형태가 됐건 cost 함수라고 부를 수 있다는 뜻이다.
자연상수(e)의 지수를 사용하는 hypothesis의 비용을 판단하기 위해, 드디어 log가 등장했다.
나는 지수도 잘 모르고 로그도 잘 모르는데, 찾아보니 지수의 반대를 로그로 부르고 있었다.
로그를 사용하는 이유는 앞에서 잠깐 언급했던 것처럼, 구불구불한 cost 함수를 매끈하게 펴기 위함이다.
당연히 첫 번째 목표는 hypothesis가 반영된 비용을 올바르게 판단하기 위함이다.
조금 아쉬운 점은 새로운 cost 함수인 C(H(x), y)의 공식이 2가지라는 사실이다.
y가 1일 때 사용하는 공식과 0일 때 사용하는 공식이 따로 있어서, 괜히 부담스럽게 느껴진다.



그림이 조금 희미한데, 아래쪽에 그래프가 2개 있다. 매끈한 밥그릇의 왼쪽'('을 담당하는 그래프와 오른쪽')'을 담당하는 그래프.
두 개를 합쳐서 하나의 밥그릇을 만든다.
log 함수가 매끈하기 때문에 gradient descent 또한 잘 동작한다.
왼쪽은 -log(z)의 그래프이고, 오른쪽은 -log(1-z)의 그래프다. 이 부분은 수학적인 내용이 필요하므로 다시 정리하도록 하겠다.


교수님께서 y 값이 1일 때와 0일 때에 대해 손수 적어놓으셨다. 한번 따라가 보자.
----------------------------------------------
c(H(x), y) =
	y = 1이면, -log(H(x))
	y = 0이면, -log(1-H(x))
----------------------------------------------//

  y가 1일 때
      ★ H(X) = 1 일 때는 왼쪽 그래프에서 y는 0이 된다. (cost = 0) ★
      H(X) = 0일 때는 왼쪽 그래프에서 y는 무한대(∞)가 된다. (cost=무한대)

  y가 0일 때
      ★ H(X) = 0 일 때는 오른쪽 그래프에서 y는 0이 된다. (cost = 0) ★
      H(X) = 1일 때는 오른쪽 그래프에서 y는 무한대가 된다. (cost=무한대)


이번 설명에서 조금 헷갈리는 것이 y와 H(X)의 정의였다. 이전 글들에서 당연히 설명하면서 이해했다고 생각했음에도,
얕은 지식이라 새로운 게 나타나면 헷갈려 버린다.
y는 파일 등에서 가져온 실제 데이터(label)이고, H(X)는 y를 예측한 값(y hat)이다.
그래서, H(X)는 기존의 hypothesis처럼 틀릴 수 있는 가능성이 있다.

  y가 1일 때 1을 예측(H(X)=1)했다는 것은 맞았다는 뜻이고, 이때의 비용은 0이다.
  y가 1일 때 0을 예측(H(X)=0)했다는 것은 틀렸다는 뜻이고, 이때의 비용은 무한대이다.
  y가 0일 때 0을 예측(H(X)=0)했다는 것은 맞았다는 뜻이고, 이때의 비용은 0이다.
  y가 0일 때 1을 예측(H(X)=1)했다는 것은 틀렸다는 뜻이고, 이때의 비용은 무한대이다.

log로 시작하는 cost 함수가 어렵긴 해도, 맞는 예측을 했을 때의 비용을 0으로 만들어 줌으로써
cost 함수의 역할을 제대로 함을 알 수 있다.

착각하면 안 되는 것이 있다. 예측의 결과는 0과 1이 나올 수도 있지만, 대부분은 0과 1 사이의 어떤 값이 된다.
가령, 0.7이 나왔으면 0.7에서 label에 해당하는 1 또는 0을 뺀 결과를 활용하는 것은 여전히 동일하다.
다만 Linear Regression에서 했던 것처럼 페널티를 주기 위한 제곱을 하진 않는다.
log 자체에 페널티와 동일한 무한대로 수렴하는 값이 있기 때문에 제곱을 할 필요가 없기 때문이다.
★★ 결론적으로 분류(classification)에서도 지금까지와 동일하게 최소 cost가 되도록 W를 조절하는 것이 핵심이다. ★★



앞에 나온 2개의 공식은 아래에 다시 나오니까 생략하고. 마지막 공식은 가운데 있는 공식을 하나로 합친 공식이다.
하나로 만들지 않으면, 매번 코딩할 때마다 if문이 들어가기 때문에 불편하다.

두 개의 값 A와 B가 존재한다. y의 값에 따라 A와 B 중에서 하나만 사용하고 싶다.
y는 언제나 1 또는 0 중에서 하나의 값을 갖고, 1일 때 A 함수를 사용하려고 한다.
---------------------------------------------
  ★ 공식 : y*A + (1-y)*B ★
      y=1  ==>  1*A + (1-1)*B = A
      y=0  ==>  0*A + (1-0)*B = B
---------------------------------------------//

아주 간단한 공식이다. y에 대해 1 또는 0을 넣어보면, 원하는 값만 사용할 수 있음을 쉽게 알 수 있다.
A는 -log(H(x))이고, B는 -log(1-H(x))이다. 이렇게 해서 마지막 세 번째에 있는 공식이 만들어졌다.

    y*A + (1-y)*B  ==>  y * -log(H(x))  +  (1-y) * -log(1-H(x))  ==>  -ylog(H(x)) - (1-y)log(1-H(x))


앞에서 만들었던 공식을 cost 함수와 합쳤다. 일단 아래처럼 음수 기호(-)를 앞으로 빼서 안쪽을 +로 만든다.
    -ylog(H(x)) - (1-y)log(1-H(x))   ==>   -(ylog(H(x)) + (1-y)log(1-H(x)))

cost 함수인 cost(W) = (1/m) ∑ c(H(x), y)를 재구성한다.
    (1/m) ∑  -(ylog(H(x)) + (1-y)log(1-H(x)))   ==>   -(1/m) ∑ylog(H(x)) + (1-y)log(1-H(x))


이렇게 해서 최종적인 cost 함수가 탄생했다. 이 함수는 나중에 파이썬으로 직접 구현해서 보여줄거라서,
지금 이해 못했어도 아직 기회가 있다. 나 또한 이 식을 보고는 이해하지 못했다.

두 번째 있는 공식은 cost(W)에 대해 미분을 적용해서 W의 다음 번 위치를 계산하는 공식이다.
Linear Regression과 공식에서는 달라진 게 없지만, 김성훈 교수님께서 말씀하신 것처럼 cost(W)가 달라져서
매우 복잡한 미분이 되어 버렸다.
동영상에서도 이 부분을 생략하셨기 때문에, 우리 또한 "미분을 하는구나!" 정도로 넘어가야 하는 부분이다.

Linear Regression 알고리듬에서 cost 함수와 gradient descent를 구하는 공식이 각각 있었던 것처럼
Logistic Regression에서도 두 개의 공식이 있어야 하고, 위의 그림에서 확인할 수 있었다.

다만 미분에 대한 공식은 달라졌지만, W를 일정 크기만큼 이동시키는 부분은 여전히 동일하기 때문에 앞서 배운
gradient descent 알고리듬을 여기서도 사용할 수 있고, 다음 글에서 텐서플로우를 통해 보여줄 것이다.



===============================================================================================================
개인적으로는 코딩이 훨씬 쉽고 개념 정리하는 것도 쉽게 느껴진다.
힘들게 힘들게 Logistic Regression에 대해 이론적인 내용을 정리했으니, 텐서플로우로 결과를 볼 때가 됐다.


앞의 글에서 엄청난 설명을 했던 공식들이다. 복잡하긴 한데, 앞의 글을 읽었다면 그래도 좀 알 것 같은 느낌이 들어야 하지 않을까?


'''


import tensorflow as tf
import numpy as np

# 04train.txt
# #x0 x1 x2 y
# 1   2   1   0
# 1   3   2   0
# 1   3   5   0
# 1   5   5   1
# 1   7   5   1
# 1   2   5   1

# 원본 파일은 6행 4열이지만, 열 우선이라서 4행 6열로 가져옴
xy = np.loadtxt('07_Logistic Classification_02_cost함수와 gradient decent.txt', unpack=True, dtype='float32')
print('xy ==>', xy)
'''
출력
---------------------------------------------
[[1. 1. 1. 1. 1. 1.]
 [2. 3. 3. 5. 7. 2.]
 [1. 2. 5. 5. 5. 5.]
 [0. 0. 0. 1. 1. 1.]]
---------------------------------------------//
'''
# print(xy[0], xy[-1])        # [ 1.  1.  1.  1.  1.  1.] [ 0.  0.  0.  1.  1.  1.]

x_data = xy[:-1]            # 3행 6열
y_data = xy[-1]             # 1행 6열

X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

# 가중치 배열을 생성하는 코드로 1행 3열의 2차원 배열이다.
# feature별 가중치를 난수로 초기화. feature는 bias 포함해서 3개. 1행 3열.
W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))

'''
X는 3행 6열의 배열이다. 현재는 placeholder로 되어 있고, 맨 밑의 반복문에서 데이터와 연결된다. 
matmul 함수는 행렬 곱셈을 지원하는 함수로, W와 X의 행렬 크기가 어울리지 않으면 에러를 발생시킨다. 
여기서 사용된 2개의 변수에는 오해가 있을 수 있다. 
정확하게 말하면 h가 hypothesis에 해당하고, hypothesis는 sigmoid에 해당한다. 
그러나, 처음에 나온 그림에서 첫 번째 공식을 표현할 때 sigmoid를 H(X)로 표현했기 때문에, 
김성훈 교수님의 동영상에서는 적절한 변수 이름이라고 할 수 있다. 
그러나, sigmoid의 역할이 자연상수 e의 지수를 사용해서 
hypothesis의 결과를 0과 1 사이의 값으로 변환하는 것임을 상기하면 수긍 가능할 것이다.

'''
# 행렬 곱셈. (1x3) * (3x6)
h = tf.matmul(W, X)
hypothesis = tf.div(1., 1. + tf.exp(-h))    # exp(-h) = e ** -h. e는 자연상수


# exp()에는 실수만 전달
# print(tf.exp([1., 2., 3.]).eval())      # [2.71828175 7.38905621 20.08553696]
# print(tf.exp([-1., -2., -3.]).eval())   # [0.36787945 0.13533528 0.04978707]

# 앞의 그림에서 두 번째 공식을 표현한다.
# reduce_mean 함수의 파라미터는 정확하게 시그마(∑)의 오른쪽에 있는 공식을 있는 그대로 표현하고 있다.
# 글자 그대로 log 함수를 호출했고, 약속했던 매개변수를 전달한다. 코드 맨 앞에는 음수 기호(-)도 붙어 있다.
#
cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))

rate = tf.Variable(0.1)
optimizer = tf.train.GradientDescentOptimizer(rate)
train = optimizer.minimize(cost)

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for step in range(2001):
    sess.run(train, feed_dict={X: x_data, Y: y_data})

    if step == 0:
        print('step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W)')
        print('=====================================================================')

    if step % 100 == 0:
        # if step % 20 == 0:
        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))

print('-----------------------------------------')
'''
학습 결과를 사용해서 성공과 실패를 예측하고 있다. 
여기서 착각하면 안 되는 것이 hypothesis가 성공 또는 실패를 참과 거짓으로 직접 알려주는 것이 아니라 
sigmoid 함수를 통해 0과 1 사이의 값으로 변환한 결과를 알려준다는 사실이다. 
그래서, 참과 거짓에 대한 결과는 직접 0.5와 비교해서 사용해야 한다. 
60%의 성공 확률이라고 표현할 수도 있기 때문에, 이렇게 반환하는 것이 맞다고 생각한다.

placeholder에 전달되는 X의 값이 조금 어렵다. 
그런데, 앞에서 '07_Logistic Classification_02_cost함수와 gradient decent.txt' 파일을 읽어올 때의 
x_data는 6개의 데이터를 갖고 있어서 3행 6열이었다. 


★
이번에는 데이터를 1개만 전달하기 때문에 3행 1열이 되어야 하니까, 
[[1], [2], [2]]와 같은 []가 두 번 중첩되어서 나오는 것이 맞다.
★


'''
# 결과가 0 또는 1로 계산되는 것이 아니라 0과 1 사이의 값으로 나오기 때문에 True/False는 직접 판단
print('[1, 2, 2] :', sess.run(hypothesis, feed_dict={X: [[1], [2], [2]]}) > 0.5)
print('[1, 5, 5] :', sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)
print('[1, 4, 2] [1, 0, 10] :', end=' ')
print(sess.run(hypothesis, feed_dict={X: [[1, 1], [4, 0], [2, 10]]}) > 0.5)
sess.close()

'''
출력
------------------------------------------------------------
1900 0.34016454 [[-5.930599    0.47584438  1.0310678 ]]
2000 0.33914974 [[-6.0297337   0.48023096  1.0484238 ]]


최종적으로 두 번째 열에 있는 cost는 0이 되지 않았다. 앞에서 보여준 간단하면서도 feature가 하나밖에 없는 예제에서는 
cost가 0이 될 수도 있겠지만, 실제 상황에서는 0이 될 수 없는 것이 맞다. 
10만번을 구동시켜서 확인했는데, 위와 같은 결과가 나왔다. 줄어들기는 하지만, 0이 될 수는 없다.
결국  classification이라고 하는 것은 2차원 좌표상에 흩어진 데이터를 직선을 그어서 구분하겠다는 뜻인데, 
그 직선을 계산하는 비용이 0이 될 수는 없다. 어떻게 계산해도 상당한 거리일 수밖에 없다. 
Logistic Regression을 포함한 모든 예측에서 100% 만족한 결과란 존재하지 않는다. 
100%는 신의 영역이다. 대통령 선거와 같은 투표 결과 또한 위아래 5% 정도의 오차를 허용하고 있지 않은가?

------------------------------------------------------------//
'''
