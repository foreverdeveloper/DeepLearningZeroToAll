'''
이전에 다룬 내용은 좌표상에 위치한 데이터를 가로지르는 직선을 그어서 새로운 데이터의 위치를 예측하는 Linear Regression 모델이었다.
물론 feature가 여러 개인 multi-variables인 경우에는 당연히 직선으로 표시되지 않겠지만, 나에게는 이 정도의 설명이 딱 좋은 것 같다.

이번에 공부한 내용은 Linear Regression을 활용해서 데이터를 분류하는 모델이다.
이름은 Logistic Classification이라고 부른다. 분류에서 가장 단순한 모델로 2가지 중에 하나를 찾는 모델이다.

두 가지 분류를 활용할 수 있는 몇 가지 예제를 설명하고 있다.
스팸 메일 탐지, 페이스북 피드 표시, 신용카드 부정 사용은 두 가지 값 중의 하나를 선택하게 된다.
프로그래밍에서는 이 값을 boolean이라고 부르지만, 여기서는 쉽게 1과 0으로 구분한다.
1은 spam, show, fraud에 해당한다. 1과 0에 특별한 값을 할당하도록 정해진 것은 아니다.

★★다만 찾고자 하는 것에 1을 붙이는 것이 일반적이다.★★

이번 그래프는 학생들의 성적을 선처리(preprocessing)해서 모든 점수를 0 또는 1로 변환했단는 것을 전제로 한다.
linear regression은 Wx+b 공식을 통해서 직선을 긋고 이걸 토대로 결과를 예측하는 방식이다.
교수님께서는 기존 그래프에 공부를 많이 한 학생이 등장하면 기울기가 달라지기 때문에 문제가 발생한다고 말씀하셨다. 이걸 풀어보자.

예측을 한다는 것은 학습된 모델에 새로운 데이터를 전달한 결과를 가져온다는 뜻이다.
한 번 학습된 모델을 지속적으로 사용하면서 활용하는 것이 일반적이다. 그럴려면 학습된 모델을 수정하면 안 되는데,
너무 많이 공부한 학생이 발견되어서, 즉 기존 모델로 예측에 실패해서 모델을 수정하게 되었다는 것을 의미한다.

1. 학습된 모델을 수정하는 것은 옳지 않다.
  예측이 틀릴 수는 있지만, 누구나 알 수 있는 사실에 대해 틀린 예측을 하면 올바른 모델이라고 할 수가 없다.
2. 어떤 방식의 선처리가 됐건 데이터를 2개로 분리하는 것은 편협한 느낌일 수밖에 없다.
  0에서 100점, 0에서 1.0 등의 구간을 통해서 성공과 실패를 판정할 수 있는 모델이 필요하다.
3. 직선에 대해 x에 대한 y를 예측하는 것이 아니라 직선의 아래쪽과 위쪽이라는 새로운 방식의 판정 기준을 도입해야 한다.
  이 부분은 분류이기 때문에 어쩔 수 없을 수도 있다.
4. 점수가 아주 작거나 매우 큰 경우에 대해서도 모델을 수정하지 않고 사용할 수 있는 방법이 필요하다.
  이 부분은 1번과 같은 말이 될 수도 있다.


Linear Regression을 분류에 사용할 때 발생할 수 있는 문제에 대해서는 앞에서 설명했다.
Wx+b라는 공식을 있는 그대로 사용하면 W를 1/2이라고 했을 때, x의 값이 100인 경우 50이라는 엄청난 값이 만들어 질 수 있다.
0과 1만을 사용해야 하는데, 범위를 벗어나는 값이 나오게 된다.
50보다 작으면 0, 크면 1이라고 표현하거나 1/2보다 작으면 0, 크면 1이라고 표현할 수 있는 추가 코드가 반드시 있어야 한다.
아래 그림에서는 이러한 표현식을 sigmoid라고 설명하고 있다.


시그모이드(sigmoid) 함수는 앞에서 배운 공식 (Wx + b )이 만들어 내는 값을 0과 1 사이의 값으로 변환한다.
어떤 값이든지 sigmoid  함수를 통과하기만 하면 0과 1 사이의 값이 되는 놀라운 기적을 보여준다.
그림과 공식을 보면 그냥 겁먹게 되는데, 그럴 필요 없다. 소스 코드를 보면 진짜 간단하다. 왼쪽 그림의 공식을 보자.


e로 시작하는 계산식이 0일 때, 1/1이 되어서 최대값인 1이 된다.
e로 시작하는 계산식이 매우 클 때, (1/큰수)이 되어서 최소값인 0이 된다.
WX가 0일 때, 지수가 0이 되어, 분모는 2가 되고, 이때 중간값인 1/2이 된다.

H(X) =  1/ ( 1 + e **((-W**T) * X )


e로 시작하는 공식이 복잡해 보이지만, 실제로는 전혀 복잡하지 않다.

  e는 자연상수(mathematical constant) 또는 오일러 상수라고 부르고, 2.718281828459로 시작하는 무한 소수.
  e를 사용하는 이유는 이걸 사용하면 공식이 매우 자연스러워지면서 짧게 표현할 수 있기 때문이다. (어느 수학자)

결국 이 식은 e의 지수(exponent)일 뿐이다. Wx+b를 그래프에서 표현하는 것처럼 z라고 부르자.
z가 음수가 되면 e의 z승은 엄청나게 작아지고, 양수가 되면 엄청나게 커진다.
z 앞에 음수 기호가 있기 때문에 z가 음수일 때 오히려 양수가 되어 e의 z승은 큰 값이 되고,
분모로 사용되었기 때문에 전체 값을 0에 가깝게 만들어 버린다.
어찌 됐든 이 공식은 z가 무한히 작거나 무한히 커도 잘 동작한다는 것은 기억하자.

아주 중요하니까, 다시 한번 강조한다.
sigmoid는 linear regression에서 가져온 값을 0과 1 사이의 값으로 변환한다.
★ z가 0일 때, 0.5가 된다. ★


'''


import math
import numpy as np

# 어떤 숫자가 들어와도 0과 1 사이의 값으로 변환해주는 함수
# 여기서는 100, 0, -10의 세 가지를 사용했다. 0을 넣었을 때,
# 가운데 값이므로 정확하게 0.5가 나와야 하는 것도 중요하다.
# z는 값(scalar)일 수도 있고, vector 또는 matrix일 수도 있다.
# math 모듈에 자연상수 e가 들어있는 것을 찾았다. 지수 연산자인 **를 사용해서 아주 쉽게 분모를 구성
# 파이썬3에서는 //는 정수 나눗셈, /는 실수 나눗셈
# sigmoid는 나중에 행렬까지 처리할 수 있어야 하기 때문에 numpy의 배열을 사용해서 검증
# 리스트는 행렬 연산을 지원하지 않는다.

def sigmoid(z):
    return 1 / (1 + math.e ** -z)

print(sigmoid(100))
print(sigmoid(  0))
print(sigmoid(-10))
print(sigmoid(np.array([100, 0, -10])))
'''
출력
-------------------------------------------------
1.0
0.5
4.539786870243442e-05
[1.00000000e+00 5.00000000e-01 4.53978687e-05]
-------------------------------------------------//
'''
