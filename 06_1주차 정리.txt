여기까지가 1주차에 진행한 내용이다.

이후에 나오는 내용에 비해 많은 내용인데다 처음으로 진행하는 스터디라 어려운 점이 많았다.
스터디를 준비하는 과정에서 궁금한 내용이 많아서 적어둔 것이 있었다.
지나고 보니, 별거 아니었고 대부분은 설명할 수 있게 되었다.
말도 안 되는 궁금증일 수 있지만, 나와 같은 사람들이 있을 수 있어서 아는 범위에서 답을 남긴다.

1. Variable
    우리 말로는 변수라고 하는데, 이쪽 분야에서는 입력이라고도 한다. 중요한 점은 이게 변한다는 거다.
    x축의 데이터가 변함에 따라 결과로 나오는 y도 변하게 된다.
    feature와 같은 뜻이고, 앤드류 교수님 수업에서는 feature를 주로 사용한다.
    입력 변수는 앤드류 교수님은 몇 천 개도 얘기하시던데.. 나는 감이 안 잡힌다.

2. label
    답을 갖고 있는 변수를 가리킨다. 여기서는 김성훈 교수님 설명에서는 y_data에 해당한다.
    답을 알고 있으면 supervised, 모르면 unspervised가 된다.

3. cost 함수 vs. 분산
    통계 수업을 잠깐 들었을 때, 분산과 표준편차에 대해 배웠다.
    이게 머신러닝에 쓰이는 것은 아직 모르겠지만,
    비용을 계산한다는 것이 결국 분산이나 표준편차와 같은 느낌이다.
    데이터가 분포한 모습을 보는 것이나 그 모습에서 비용을 계산하는 것이나 정도의 차이처럼 보였다.

4. linear gradient 알고리듬에서 learning rate를 처음엔 크게, 나중엔 작게 할 수 없을까?
    목표에 도달하기 위해 처음엔 빨리 가다가 어느 시점부터는 미세하게 다가가면 좋겠다는 생각이 들었다.
    당연히 learning rate에 해당하는 rate 변수의 값을 바꾸면 할 수 있다. 얼마든지.
    그런데, 할 필요가 없다.
    아래 그림을 보면, 동일한 rate를 사용함에도 불구하고 최소 비용에 가까워질수록 작게 이동하는 것을 알 수 있다.
    W가 1일 때의 주변 빨강 점이 오밀조밀하다.

5. 데이터가 몇 개 없는데, 너무 오래 걸린다.
    텐서플로우가 무겁기 때문인지 기다리는 것이 느껴진다.
    데이터가 100개 정도 되고 변수가 5개 정도 되면, 화장실에 갔다올 정도가 될 수도 있다.
    변수가 많아진다는 것은 반복문을 여러 개 사용하는 것과 같다는 것을 알 수 있다.
    그래도 반복문을 여러 번 중첩시켜도 데이터가 없는데.. 너무 느리다.
    그냥 텐서플로우가 일단은 무거운 것 같다.

6. random_uniform 호출 결과를 직접 보려면?
    "02. TensorFlow의 설치 및 기본적인 operations (lab 01)" 글에서 설명했다. showTensor 함수를 봐라.
    데이터가 어떻게 생긴지 알아야 코드를 이해할 수 있는데, 이걸 몰라서 많이 답답했었다.

7. gradient descent 알고리듬에서 어떻게 최소값을 찾는지 모르겠다.
    이게 제일 문제였었다. 아무리 봐도 이해가 가지 않았었다.
    현재 비용(cost)에서 기울기를 구해 다음 번 비용으로 이동하는 게 전부였다.
    이때 내부적으로 기울기에 해당하는 W를 미분으로 계산한 값을 빼서 다음 번 W를 계산하는 방식이다.
    이 설명에서는 현재 값이 얼마인지는 필요치 않으므로, 우리는 어디에 있건 최소 비용을 찾아갈 수 있게 된다.
    앞의 W와 cost 그래프를 다시 한번 보도록 하자.
    그래도 안되면 파이썬으로 구현해 놓은 "06. cost 함수를 파이썬만으로 직접 구현"을 읽어 본다.

8. cost 함수에서 W가 1일 때의 뜻은?
    김성훈 교수님께서 잘 정리해 주셨다. 그래도 다시 정리해 보면,

      hypothesis = w * x_data + b
      mean(square(hypothesis - y_data))

    x를 hypothesis에 집어넣고 y를 뺀 결과의 제곱을 리스트 전체에 대해 합계 계산, 그리고 평균 계산

      ((1*1 - 1)**2 + (1*2 - 2)**2 + (1*3 - 3)**2) / 3 = (0**2 + 0**2 + 0**2) / 3 = 0
      ((0*1 - 1)**2 + (0*2 - 2)**2 + (0*3 - 3)**2) / 3 = (1**2 + 2**2 + 3**2) / 3 = (1+4+9) / 3 = 14/3 = 4.67
      ((2*1 - 1)**2 + (2*2 - 2)**2 + (2*3 - 3)**2) / 3 = (1**2 + 2**2 + 3**2) / 3 = (1+4+9) / 3 = 14/3 = 4.67

9. tf.Variable은 모델 안에서 계속 변하기 때문에 변수가 아닐까?
    tf.constant는 연산 과정에서 바뀌지 않는 것을 뜻하고, tf.Variable은 바뀔 수 있음을 뜻한다.
    바뀔 일이 없을 것 같은 learning rate을 tf.Variable로 주는 이유가 내부적으로 바뀔 가능성이 있기 때문이다.
    실제로도 바뀐 것을 본 적이 있다.
    W와 b가 바뀌는 이유는 tf.Variable로 주었기 때문이고, 바뀌어야 하기 때문에 반드시 tf.Variable이어야 한다.

10. Session.run()
    공식 도움말에 보면 runs one "step" of TensorFlow computation라고 되어 있다.
    코드에서는 update 또는 cost라는 이름의 변수로 표현되는데,
    run(update)는 update에 연결된 모든 변수가 1회 실행한 만큼 변화한다는 뜻을 담고 있다.
    다른 말로는 다음 값으로 넘어간다라고 얘기할 수도 있겠다. 우리 코드에서 변화한 것은 기울기(W)와 y 절편(b)이었다.

11. one-variable과 multi-variables의 hypothesis 공식이 왜 똑같을까?
    값을 하나씩 처리하는 방식이라면 당연히 달라야 한다.
    그러나, 데이터 갯수에 상관없이 행렬로 처리하기 때문에 같을 수밖에 없다.
    one-variable 방식에서는 변수가 하나였지만, 실제로는 1x1 행렬을 사용한 것이었다.
    multi-variables 방식에서는 1xn 크기의 행렬을 사용했기 때문에 코드는 같다.

12. gradient descent 알고리듬에서 local optimum에 있을 때, 왜 값이 더 아래로 내려가지 않을까?
    local optimum 아래에 global optimum에 해당하는 최저점이 위치할 수도 있지만,
    현재 cost가 지속적으로 내려갈 수 있는 기울기를 제공하지 않는다면, 진행할 수가 없다.
    local optimum이라는 것은 나름 조그마한 밥그릇을 형성했다는 뜻이므로 내려갈 수 없는 것이 당연하다.

13. 기울기가 0이 되는 시점에서 텐서플로우는 무엇을 할까?
    텐서플로우를 구동해 보면, 최소 비용이 되어서 같은 결과를 계속해서 보여주게 된다.
    이때 텐서플로우는 내부적으로 step에 따른 결과를 매번 계산할까?
    오픈소스이긴 하지만, 코드를 살펴볼 생각이 없는 나에겐 영원히 풀 수 없는 미스테리!

14. minimize cost(W, b)의 뜻은?
    가장 작은 기울기(W)와 y 절편(b)을 구하는 것이 아니라 최소 비용에 따른 W와 b를 찾으라는 뜻이다.
    최소 비용(minimize cost)에 W와 b를 파라미터로 전달해서 이름 그대로 최소 비용을 찾겠다는 뜻이다.

15. 텐서플로우의 노드(node)와 간선(edge)
    텐서플로우는 Data Flow Graph, 즉 데이터의 흐름을 그래프로 표현한 것.
    node와 edge로 구성되어 있고, node는 operation, edge는 n-dim data array(tensors) 표현.
    일반적으로는 node를 데이터, edge를 연산으로 생각한다.

16. y 절편 b는 몇 번 더해지는가?
    이 부분에 대한 답은 "08. multi-variable linear regression을 TensorFlow에서 구현하기(lab 04)" 글에 있다.
    x_data는 여러 개의 feature가 있어 복잡하니까, y_data의 갯수만큼이라고 얘기하면 쉽다.
    y_data = [1, 2, 3, 4, 5]라면 b는 다섯 번 더해진다.
    y 절편이라고 하는 것이 원점(0, 0)에서 시작하는 직선을 y 절편만큼 이동시켰다는 뜻이고,
    이것은 직선에 포함된 모든 데이터가 y 절편만큼 움직였다는 뜻이다.

17. rate 또는 alpha라고 부르는 learning rate의 뜻은?
    현재 cost에 대한 기울기(W)는 이미 정해져 있다.
    이 값은 외부에서 변경할 수 없고, 내부적으로 gradient descent 알고리듬을 통해 스스로 수정해 나간다.
    이렇게 자동으로 구한 값은 일반적으로 너무 커서 overshooting이라는 문제를 일으킬 확률이 매우 높다.
    경사를 타고 내려가는 것이 아니라 거꾸로 올라가는 현상을 말한다.
    그래서, 보통은 1보다 작은 값을 곱해서 오버슈팅이 일어나지 않고 경사를 타고 내려가도록 조정한다.
    이 값은 정해져 있는 것이 아니라 데이터에 따라 달라지므로, 여러 번에 걸쳐 테스트를 통해 결정하게 된다.
    꽤 빠른 속도로 내려가서 최저 비용을 계산한다면 이상적인 learning rate라고 부를 수 있다.
    이 부분은 뒤의 동영상에 나오는데, 이상적인 learning rate은 여러 번 구동시켜서 찾는 방법 외에는 없다.

18, Wx + b에서 W를 찾는 것은 알겠다. 그런데, 어떻게 b까지 찾을 수 있는 것일까?
    결국엔 W와 b를 모두 바꾸면서 비용을 측정하는 수밖에 없다.
    그래서, 아래쪽에 답을 찾지 못한 질문에 있는 것처럼  b가 추가되면 multi-variables와 같은 상황이 된다.
    기울기를 바꾸나 y 절편을 바꾸나 계속 바꿔야 하는 것은 같다.
    기울기 바꾸는 방법은 알고 있으니, 오히려 y 절편을 바꾸는 것이 더욱 어려워 보인다.

19. 학습한 결과를 저장했다가 내일 사용하는 방법은?
    Saver 클래스에 포함된 save와 restore 함수를 통해 쉽게 구현할 수 있다.
    save 함수를 호출하면 checkpoint 파일이 생성되는데, 이 파일을 restore하면 모델을 재사용할 수 있게 된다. 확장자는 ckpt.

20. 저장할 수 있다면, 크기는 어느 정도일까?
    모델만 놓고 판단한다면 ckpt 파일의 크기면 충분하다.
    지금까지의 설명을 적용하면, W와 b만 알고 있으면 되기 때문에 20바이트만 있어도 충분할 수 있다.
    다만 실제 상황에서는 feature 개수나 모델을 구성하기 위한 필터 등의 환경이 필요하기 때문에 좀더 많이 필요할 수 있다.
    그렇지만 학습이 끝났다면 연산과 메모리 모두 거의 필요없는 수준이라고 볼 수 있다. 학습과 비교한다면.

21. convex 형태인지 확인할 수 있는 방법이 있을까?
    동셩상에서는 확인해야 한다고만 하셨고, 방법에 대한 말씀은 없으셨다.
    내 생각에는 "있다"라기 보다는 이미 만들어진 방법을 사용하고 있다고 보는 것이 맞는 것 같다.
    딥러닝의 핵심은 gradient descent 알고리듬에 있고, 이 방법은 cost에 대해 convex하다는 것을 항상 보장한다.
    gradient descent 알고리즘의 여러 변형이 존재하지만,
    모두 cost에 대한 미분을 적용하기 때문에 convex하다는 사실은 여전히 달라지지 않는다.

    다만 기존에 알려진 방법이 아닌 새로운 방법으로 모델을 구성하고 있다면
    반복 횟수를 늘리거나 learning rate을 조절해서 convex 형태를 직접 확인해볼 수도 있을 것 같다.
    만약 그런 일이 생긴다면 무척이나 괴로울 것 같다. ^^

22. multi-variables linear regression에서 hypothesis는 등고선 모양에서 중심을 향해 가는 직선일까?


    왼쪽 그림은 동영상에 나왔던 그림이고, 오른쪽 그림은 앤드류 교수님 수업에 나오는 그림이다.
    양쪽 모두 feature가 2개인 경우를 표현하고 있는데, 결국은 가운데 있는 global optimum을 찾아가야 한다.
    이때 최저점을 찾기 위해 임의의 위치에서 직선으로 내려가는 방법이 존재할까?

    프로그램적으로는 가능하지 않지만, 왠지 수학적으로는 가능해 보인다.
    혹은 직선에 가까운 형태 정도는 존재하지 않을까, 생각한다.
    천천히 생각해 보면 직선으로 내려간다는 것이 불가능해 보일 것이다.

    뒤쪽 동영상에 김성훈 교수님께서 특정 사이트를 통해
    여러 가지 방법들의 성능에 대해서 비교해 주는 부분이 있는데, 거기에 힌트가 있긴 한듯 하다.
    모든 방법들이 직선으로 향하지 않고 떨듯이 움직이면서 둥글게 선회하기도 하고 다양한 모습을 보여준다.


