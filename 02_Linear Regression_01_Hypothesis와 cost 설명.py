'''
회귀분석
    점들이 퍼져있는 형태에서 패턴을 찾아내고, 이 패턴을 활용해서 무언가를 예측하는 분석.
    새로운 표본을 뽑았을 때 평균으로 돌아가려는 특징이 있기 때문에 붙은 이름.
    회귀(回歸 돌 회, 돌아갈 귀)라는 용어는 일반적으로 '돌아간다'는 정도로만 사용하기 때문에
    회귀로부터 '예측'이라는 단어를 떠올리기는 쉽지 않다.


Linear Regression
    2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델.
    머신러닝 입문에서는 기본적으로 2차원이나 3차원까지만 정리하면 된다.
    여기서는 편의상 1차원 직선으로 정리하고 있다.
    xy축 좌표계에서 직선을 그렸다고 생각하면 된다.


Hypothesis
    Linear Regression에서 사용하는 1차원 방정식을 가리키는 용어로, 우리말로는 가설이라고 한다.
    수식에서는 h(x) 또는 H(x)로 표현한다.

    H(x) = Wx + b에서 Wx + b는 x에 대한 1차 방적식으로 직선을 표현한다는 것은 모두 알 것이고,
    기울기에 해당하는 W(Weight)와 절편에 해당하는 b(bias)가 반복되는 과정에서 계속 바뀌고,
    마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측(prediction)에 사용하게 된다.
    최종 결과로 나온 가설을 모델(model)이라고 부르고, "학습되었다"라고 한다.
    학습된 모델은 배포되어서 새로운 학습을 통해 수정되기 전까지 지속적으로 활용된다.

    우리가 찾으려는 직선은 모든 데이터를 관통하면 최상이겠지만,
    들쭉날쭉한 데이터에 대해 그런 상황은 거의 불가능하다.
    그래서, 데이터에 가장 가깝게 그려지는 직선을 찾는 것이 목표가 되고,
    직선으로부터 각각의 데이터(좌표)까지의 거리 합계를 계산한 것을 cost라고 부르고,
    이 값이 가장 작은 직선을 찾으면 목표 달성이다.


Cost (비용)
    Hypothesis 방정식에 대한 비용(cost)으로 방정식의 결과가 크게 나오면 좋지 않다고 얘기하고,
    루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정하게 된다.
    놀랍게도 미분이라는 수학 공식을 통해 스스로 최저 비용을 찾아가는 마술같은 경험을 하게 될 것이다.
    프로그래밍을 하는 사람의 입장에서 어떻게 이런 일이 생길 수 있을까.. 라고 생각할 정도로 놀라운 경험이었다.
    그런데, 여전히 신기하다!
    H(x) = Wx + b

    H(x) - y
    y를 빼는 행위에는 H(x)에 포함된 모든 x만큼의 계산이 포함되어 있다

    cost = (1/m) ∑ (  H(x) - y ) ** 2

    cost(W, b) = (1/m) ∑ (  H(x) - y ) ** 2

    Goal: Minimize cost, 이 비용을 최소로 만드는 W(기울기)와 b(절편)를 찾는 것이 목적이다
	    minimize cost(W, b)
	    cost(W,b)로 시작하는 공식을 보면, 실제로는 W와 b가 보이지 않는다. H(x) 안에 숨어있기 때문이다.


    왜 hypothesis의 결과에서 y를 뺀 다음에 제곱을 하는 것일까?
        1. 뺄셈을 하게 되면 직선 위치에 따라 음수와 양수가 섞여서 나오게 된다. 계산이 피곤해진다.
        2. 절대값을 취하는 것이 가장 쉽지만, 제곱을 하는 것도 방법이다.
            (음수x음수)와 (양수x양수)에 대해 항상 양수가 나오니까
        3. 제곱을 하면 가까운 데이터는 작은 값이 나오고, 멀리 있는 데이터는 큰 값이 나오기 때문에
            멀리 있는 데이터에 벌점(penalty)을 부과할 수 있다.

    최종적으로는 좌표가 3개 있으니까, 3으로 나눈다. 왜 나누냐고 할 수도 있지만,
    나누지 않으면 데이터가 100개만 되도 값이 엄청나게 커지게 되고 이후 계산이 복잡해진다.
    평균을 내기 위해 합계를 구한 다음에 갯수로 나누는 것과 같다고 보면 된다.

    거리의 제곱을 취한다고 얘기했는데, 이 방법을 LSM(Least Square Method)이라고 부른다.
    통계학 관련서적에 보면, 절대값을 이용한 처리보다 튼튼(robust)하다고 알려져 있다.
    이 부분에 대해서는 통계에 대한 지식이 약해 설명을 생략하지만,
    최소 제곱을 사용한 방법이 더 좋은 결과를 낸다고만 이해하고 있다.
    그리고, 표준편차를 이용한 방법도 가능할수 있는데, LSM에 비해 미분하기가 어렵다는 단점이 있다고 한다.


    f(x) = 3x+2라는 식에서 f(x) 안에 포함된 x가 무엇이냐에 따라 오른쪽 식의 결과가 달라지는 것처럼
    cost(W,b)를 이해할 때 W와 b가 바뀔 때마다 오른쪽 식의 결과가 달라진다고 생각해도 좋다.
    즉, W와 b를 공식에 적용하는 과정에서 cost가 줄어들도록 W와  b를 변경하는 것이 진짜 중요하다.


'''

import tensorflow as tf


# x와 y 데이터가 모두 1행 3열의 리스트이다.
# 2차원 좌표로 표현하면 (1,1), (2,2), (3,3)이 되고, 원점에서 시작하는 y = x라는 직선을 그릴 수 있다.
# x와 y의 갯수가 다르다는 것은 있을 수 없다. 2차원 좌표로 표현해야 되니까.
# 실수 대신 정수 데이터를 사용하기 위해서는 데이터와 변수를 함께 바꾸어야 한다.
# 그럼에도 여전히 난수의 내부 타입은 float32를 사용해야 한다. int32 등의 자료형은 에러.
x_data = [1., 2., 3.]
y_data = [1., 2., 3.]

# cost 함수에서 궁극적으로 찾고자 하는 기울기(W)와 y 절편(b)의 초기값을 설정한다.
# tf.random_uniform 함수는 정규분포 난수를 생성하는 함수로, 배열의 shape, 최소값, 최대값을 파라미터로 사용한다
# 여기서는 [1], -1.0, 1.0을 전달했기 때문에 -1에서 1 사이의 난수를 1개 만든다.
# 결과는 1행 1열의 행렬이 된다. 나중에 여러 개의 데이터를 생성하는 코드가 나오는데,
# 5개일 경우 첫 번째 파라미터로 [5]라고 전달하면 된다.
# 여기서 난수를 사용하는 중요한 의도가 있다. 최저 비용을 스스로 찾아가야 하는데,
# 시작 위치가 매번 달라짐에도 불구하고 항상 최저 비용을 찾는다는 것을 보여주기 위해서.
# 머신러닝은 초기값으로 무엇이 주어지건, 비용이 줄어드는 방향으로 스스로 진행하는 놀라운 능력을 갖고 있다.
# 정확하게 위의 사실을 확인하고 싶다면, 난수 대신 상수를 전달해도 된다. 나는 각각 5, 15, 115의 세 번에 걸쳐 검사했고, 정상적으로 동작함을 확인했다.


# W = tf.Variable(5.)     # 5, 15, 115로 테스트
# b = tf.Variable(5.)

# W = tf.Variable(15.)     # 5, 15, 115로 테스트
# b = tf.Variable(15.)

# W = tf.Variable(115.)     # 5, 15, 115로 테스트
# b = tf.Variable(115.)

# try to find values for w and b that compute y_data = W * x_data + b
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

# 정수 생성의 경우 2**n 규칙을 따르는 것이 좋다.
# W = tf.Variable(tf.random_uniform([1], 0, 32, dtype=tf.float32))
# b = tf.Variable(tf.random_uniform([1], 0, 32))

print('W ==> ', W, 'b ==>', b)


# H(x) = Wx + b라는 공식을 충실하게 구현한 코드.
# 먼저 W와 x_data를 곱한 결과는 무엇일까? int 또는 float으로 표현할 수 있는 1개의 값일까? 땡!
# 데이터를 다루는 곳에서는 이것을 벡터(vector) 연산이라고 부른다.
# W는 1x1, x_data는 1x3 행렬이기 때문에, 행렬 연산에 따라 결과는 1x3의 행렬이 나온다.
# 이해가 안 가면 행렬의 곱셈을 구글링 해보기 바란다.
# 여기에 b를 더하면 b는 1회 더해지는 것일까?
# 땡! 행렬과 행렬이 아닌 값을 더하거나 곱할 때는 행렬의 모든 요소에 영향을 주기 때문에 덧셈 또한 3회 발생한다.
# 그러나, 아직 run 함수를 호출하지 않았기 때문에 계산이 일어난 상태는 아니다.

# my hypothesis
hypothesis = W * x_data + b


# 기울기(W)와 y 절편(b)에 대한 적합성을 판단하는 정말, 정말, 정말 중요한 코드다.
# 머신러닝을 좀 한다고 얘기하려면, 텐서플로우 없이 파이썬 만으로 이 코드를 구성할 수 있어야 한다.
#   스탠포트 대학교의 앤드류 응 교수님 말씀.
# 코드가 아니라 공식을 풀어보면,
# 1. hypothesis 방정식에서 y 좌표의 값을 빼면, 단순 거리가 나온다.
#    hypothesis - y_data가 여기에 해당하고 hypothesis와 y_data 모두 1x3 매트릭스. 즉, 행렬(벡터) 연산.
# 2. 단순 거리는 음수 또는 양수이기 때문에 제곱을 해서 멀리 있는 데이터에 벌점을 부여한다.
#    tf.square() - 매트릭스에 포함된 요소에 대해 각각 제곱하는 행렬 연산
# 3. 합계에 대해 평균을 계산한다.
#    tf.reduce_mean() - 합계 코드가 보이지 않아도 평균을 위해 내부적으로 합계 계산. 결과값은 실수 1개.
# Simplified cost function
cost = tf.reduce_mean(tf.square(hypothesis - y_data))

# minimize
rate = tf.Variable(0.1)  # learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(rate)
train = optimizer.minimize(cost)

# before starting, initialize the variables. We will 'run' this first.
init = tf.initialize_all_variables()

# launch the graph
sess = tf.Session()
sess.run(init)

# fit the line
for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print('{:4} {} {} {}'.format(step, sess.run(cost), sess.run(W), sess.run(b)))

# learns best fit is W: [1] b: [0]

'''
출력결과
    출력 결과를 보면 best fit에 해당하는 W는 1.으로, b는 5.64103e-08으로 표시됐다
    마지막에 똑같은 결과가 계속해서 나오는 이유는 두 번째 열에 출력된 cost가 0.0이 되어서 
    더 이상 비용을 줄일 수 없기 때문에 계산이 필요 없어서이다.


   0 0.015134483575820923 [0.99256444] [0.13774358]
  20 0.0007566325948573649 [0.9680524] [0.07262442]
  40 0.00028587336419150233 [0.98036253] [0.04464046]
  60 0.00010801213647937402 [0.98792934] [0.02743946]
    ..... 중 략 .....
1920 0.0 [1.] [5.64103e-08]
1940 0.0 [1.] [5.64103e-08]
1960 0.0 [1.] [5.64103e-08]
1980 0.0 [1.] [5.64103e-08]
2000 0.0 [1.] [5.64103e-08]

'''
