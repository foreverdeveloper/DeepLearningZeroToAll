'''
앞에서는 문제를 쉽게 하기 위해 입력 변수(feature)가 하나밖에 없는 단순 모델을 보여줬지만,
여기서는 여러 개의 입력 변수를 다루기 위한 전초전으로 2개의 입력을 처리하고 있다

시험 성적을 예측하기 위해 공부한 시간만으로 판단하는 것에는 무리가 있다.
가령, 텔레비전을 켜고 공부했는지, 친구들하고 함께 공부했는지, 졸린 상태로 했는지 등등
성적에 영향을 줄 수 있는 다양한 요인을 살펴봐야 한다. 여기서는 수업 참석 여부를 갖고 판단하고 있다.

왼쪽 그림에는 시간을 뜻하는 변수(x, hours)가 있고,
오른쪽 그림에는 시간(x1, hours)과 수업 참석(x2, attendance)의 두 가지 변수가 있다.
그런데, 두 가지가 됐다고 해서 겁낼 필요 없다고 교수님께서 말씀하셨다. 우리에게는 거의 차이가 없다.


feature가 1개일 때는 Wx+b로 처리할 수 있었는데, 이제는 복잡해져서 w₁x₁ + w₂x₂ + b라고 해야 한다.
다시 말해, w와 x를 feature 갯수만큼 곱하고 b를 더하면 된다.


그런데, 중요한 것은 이게 아니다. 오른쪽 그림에서 hypothesis가 달라졌음에도 불구하고 cost 함수의 정의는 달라지지 않았다는 사실이다.
이 뜻은 나중에 나오는데, 기존 코드를 그대로 사용하면 multi-variable도 처리할 수 있음을 의미한다.

동영상 중간에 행렬에 대해 짧게 얘기하는 곳이 나온다.
두 가지만 이해하면 된다. 왼쪽 그림은 행렬의 곱셈. 행렬은 앞쪽의 열 전체와 뒤쪽의 행 전체가 연산에 참여한다.
참여한 셀 각각을 곱해서 더하는 것이 기본이다.
곱셈이 성립하기 위해서는 앞 행렬의 열과 뒤 행렬의 행 크기가 같아야 한다.
이번 코딩을 진행하면서 가장 많이 실수했던 부분이다. 에러가 발생하는데, 코드상으로는 문제가 없어 보였다.
오랫동안 살펴보고 나서야 행과 열의 크기가 맞지 않는다는 사실을 발견할 수 있었다.

  2행 3열 x 3행 2열 = 2행 2열
  1행 2열 x 2행 1열 = 1행 1열
  5행 3열 x 3행 5열 = 5행 5열

간단하다. 곱셈에 인접한 2개의 숫자(왼쪽 열, 오른쪽 행)만 같으면 된다.

오른쪽 그림은 전치 행렬(transposed matrix)을 보여준다.
우리가 최종적으로 원하는 것은 행렬의 특정 열과 특정 행을 곱셈해서 더한 결과인데,
이때 행 또는 열의 순서가 맞지 않는 경우가 있을 수 있다.
앞서 말했듯이 행렬 곱셈은 열과 행이 참가해야 하기 때문인데, 이때 전치 행렬을 만들어서 곱셈을 한다.


변수가 여러 개인 cost 함수를 계산하기 편하도록 행렬(matrix)로 표현했다.
w는 1x3 행렬, x는 3x1 행렬, 결과는 1x1 행렬이다.
1행 3열 x 3행 1열은 바깥쪽 행과 열이 결과 행렬의 크기를 좌우하므로 1행 1열이 나와야 한다.
cost 함수는 기울기(w)에 따른 비용을 계산하는 함수이므로, 결과는 1개의 어떤 값이 되어야 한다.
그런데, 왜 w와 x의 행렬 크기가 다른 것일까? 다른 이유는 없고 행렬 곱셈을 적용해야 하기 때문이다.
2개의 행렬을 같은 크기인 1x3으로 만들면 곱할 수가 없기 때문에 전치(transpose)시킨 후에 곱셈을 해야 한다.
행렬은 우리가 직접 만들기 때문에 조금이라도 계산이 편하도록 미리 transpose시켰다고 보면 된다.

행렬을 hypothesis로 표현하게 되면 오른쪽 그림이 나온다.
결국은 행렬 곱셈이라는 1개의 표현으로 정리되기 때문에 지금까지 배운 hypothesis와 같을 수밖에 없다.


수학이 제일 어려웠던 것처럼 수학하는 사람들이 참, 똑똑하다.
hypothesis를 처리할 때, 계산을 쉽게 하기 위해 y 절편에 해당하는 b를 넣지 않았었다.
계산의 편리함도 사실이긴 하지만, 진실은 필요없기 때문이었다는 것을 위의 그림이 보여주고 있다.

변수가 여러 개일 때, 결국은 행렬 곱셈을 해야 하고, y 절편도 넣을 수 있기 때문에 결국 b는 공식에 나타나지 않게 된다.
다만 b는 상수이고, 최종적으로 1회 더해지기만 하면 되므로 x쪽 행렬에 1을 넣는다.
이 부분이 굉장히 중요한데 그냥 1이다. 그래야 b와 1을 곱해서 최종적으로 b를 더하게 된다.


지금까지 보여준 왼쪽 그림은 수학적으로는 어색할 수밖에 없다. 2개의 행렬을 곱하는데, 한쪽은 수평으로 길고, 한쪽은 수직으로 길다.
어색하다. 수학을 전공했던 분들은 이런 상황을 인정하지 않고, 2개의 행렬이 똑같은 크기를 갖는다고 가정한다.

가령, 1x3 행렬 2개라던가 3x1 행렬 2개라던가.. 그런데, 행렬 곱셈에서 크기가 같은 행렬은 곱할 수가 없다.
행과 열의 크기가 같은 정방 행렬(square matrix)을 제외하면 말이다.
오른쪽 그림의 공식에서는 W의 위에 T가 붙어 있다. 수학에서는 T가 붙은 행렬을 전치 행렬이라고 부른다.
다시 말해, W 행렬을 transpose해서 사용한다는 뜻이다.

여기서 잠깐! 행렬 X는 1x3이 맞을까, 3x1이 맞을까? 둘 다 맞다고 생각한다면.. 그럴 수도 있긴 하겠지만..

최종 결과는 1개의 값으로 나와야 하기 때문에, 가능한 행렬 조합은 1x3과 3x1을 곱하는 것 뿐이다.
그래서, W 행렬을 transpose시켜야 하기 때문에 정답은 3x1 행렬이 된다.



정확한 결과를 보여주기 위해 매우 확실한 데이터를 사용하고 있다. 우리의 목적은 최소 비용을 갖는 W1과 W2, b를 찾는 것이다.
그래야 정확한 직선을 그래프에 그릴 수 있고, 그걸 토대로 다른 값의 feature에 대해 예측할 수 있게 된다.
데이터에서 요구하는 정답은 W1과 W2는 모두 1이고, b는 0인 결과이다.

  1*1 + 0*1 + 0 = 1
  0*1 + 2*1 + 0 = 2
  3*1 + 0*1 + 0 = 3
  0*1 + 4*1 + 0 = 4
  5*1 + 0*1 + 0 = 5

그림에 있는 코드를 잠시 보자. 이 코드가 뒤에 나올 코드의 핵심적인 부분이다.
W1과 W2는 cost 함수에서 매번 변경된다는 것을 기억할 것이다.
시작은 -1과 1 사이의 어떤 난수로 했다. y 절편에 해당하는 b 또한 -1과 1 사이의 난수이다.

hypothesis를 보면 "1*1 + 0*1 + 0"처럼 정직하게 공식을 구성하고 있다.
이렇게 hypothesis를 구성하고 나면, 우리가 추가로 할 것은 없다. 텐서플로우에 전달하면 알아서 기울기(W)를 계산해 준다.


'''

import tensorflow as tf

x1_data = [1, 0, 3, 0, 5]
x2_data = [0, 2, 0, 4, 0]
y_data  = [1, 2, 3, 4, 5]

W1 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
W2 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

b  = tf.Variable(tf.random_uniform([1], -1.0, 1.0))

# feature 갯수만큼 곱하는 이 부분을 제외하면 one-variable과 다른 곳이 없다
hypothesis = W1*x1_data + W2*x2_data + b

cost = tf.reduce_mean(tf.square(hypothesis - y_data))

rate = tf.Variable(0.1)
optimizer = tf.train.GradientDescentOptimizer(rate)
train = optimizer.minimize(cost)

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for step in range(2001):

    sess.run(train)

    if step == 0 :
        print("step, sess.run(cost), sess.run(W1), sess.run(W2), sess.run(b)")
        print('==============================================================')

    if step%20 == 0:
        print(step, sess.run(cost), sess.run(W1), sess.run(W2), sess.run(b))

sess.close()

