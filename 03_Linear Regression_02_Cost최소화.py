'''
드디어 정말 중요한 gradient descent 알고리즘에 대한 설명이 나온다.
정말 중요하니까, 여러 번에 걸쳐 보는 것은 필수!

공식을 가볍게 만들기 위해 b를 없애는 것이 이상해 보일 수도 있지만,
뒤쪽 동영상에서는 실제로 b를 없앤다. W의 갯수가 많아지고,
행렬로 처리해야 하는 시점이 되면 b를 행렬에 넣어서 공식을 간단하게 만든다.
어차피 공식에서 사라질 값을, 우리의 이해를 돕기 위해 미리 없애주신 자상한 배려가 눈에 띈다.


b를 없앤 단순 버전의 공식에 대해 실제 데이터인 x, y를 직접 대입해서 cost를 계산해 본다.
그래프의 수평은 W, 수직은 Cost라고 되어 있다. 중요하다.

W의 값에 따라 Cost가 변하는 정도를 그래프로 보여주고 있다는 것을 명심하자.
밥그릇 모양이라고 교수님께서 말씀하셨다. 영어로는 Convex(볼록)하다고 말한다.


x와 y의 현재 데이터에서는 W가 1일 때 가장 작은 비용이 든다.
W가 1이라는 것은 H(x) = X라는 뜻이고, x에 대한 y 좌표가 정확하게 일치하기 때문에 cost는 0이 된다.
다시 말해, 이것보다 작은 비용이 발생할 수는 없으므로, 현재 구한 기울기(W)가 가장 적합하다는 뜻이 된다.

x가 세 개이기 때문에 세 번 계산해야 하고, 매번 y 값을 빼서 제곱을 한 합계에 대해 평균을 내야 한다.
개인적으로는 수식을 풀어쓴 것이 더 쉽다고 느끼지만,
요소가 100개쯤 되면 ∑(시그마)를 사용한 수식이 필요할 수밖에 없다.

W가 2일 때의 결과는 베일에 쌓여 있다. 풀어보자.
W가 2라는 뜻은 H(x) = 2X 이므로 x의 모든 값에 2를 곱한 후에 계산하면 된다.

(1/3) *((2*1-1)^2 +(2*2-2)^2 +(2*3-3)^2) = (1^2 + 2^2 + 3^2)/3 = (1+4+9)/3 = 4.67

실제 그래프를 lab 03에서 출력해서 보여주고 있으니, 정말 저렇게 나오는지 궁금하겠지만,
일단 나머지를 읽고 다음 번 글을 읽어보기 바란다.


Gradient Descent 알고리즘은 왼쪽 그림에 있는 설명처럼 다음과 같은 역할을 수행한다.

    1. 최저 비용을 계산하는 함수
    2. 다양한 최저 계산에 사용
    3. Cost 함수에 대해 최소가 되는 기울기(W)와 y 절편(b) 탐색
    4. 피처(feature, 변수)가 여러 개인 버전에도 적용 가능

위의 설명을 다시 정리해 보면, 다음과 같다.

    어떤 위치(왼쪽 또는 오른쪽 경사)에서 시작하더라도 최소 비용 계산
    기울기(W)와 y 절편( b)을 계속적으로 변경하면서 최소 비용 계산
    반복할 때마다 다음 번 gradients(기울기의 정도) 계산
    최소 비용에 수렴(converge)할 때까지 반복


공식이 변해가는 과정을 추적해 보자. 먼저 왼쪽 그림에서 데이터의 갯수가 m일 때,
m과 2m 중에서 2m을 사용하겠다는 뜻이다.
리스트에 [3, 6, 9]가 들어있다고 했을 때 이걸 데이터 갯수인 m으로 나누면 [1, 2, 3]이 된다.
그런데, 2m으로 나누게 되면 [3, 6, 9]/6이 되니까 최종 결과는 [0.5, 1, 1.5]가 된다.

제곱의 평균을 구하기 위해 사용하는 m은 W와 b를 사용해서 계산이 끝난 이후에 적용하기 때문에,
m으로 나누건 2m으로 나누건 W와 b에는 영향을 주지 않는다는 것을 먼저 이해해야 한다.

비용 계산의 목적은 최소 비용이 되었을 때의 W와 b가 필요하기 때문이다.
최소 비용이 얼마인지는 전혀 중요하지 않다.
여러 개의 비용이 있을 때, 이 중에서 가장 작은 비용을 찾고, 그 때의 W와 b를 사용하면 된다.
여러 개의 비용에 대해 모두 2를 곱하거나 2로 나누면, 비용은 늘어나거나 줄어들지만 상대적인 크기는 달라지지 않는다.


 cost(W, b) = (1/m) ∑ (  H(x) - y ) ** 2
 에서
 cost(W, b) = (1/2m) ∑ (  H(x) - y ) ** 2


그렇다면, 왜 2m을 사용할까? 오른쪽 그림에 답이 있다.
제곱을 미분하게 되면 2가 앞으로 오는데,
이때 분자에 있는 2와 분모에 있는 2를 곱해서 1로 만들면 공식이 단순해지기 때문에 일부러 넣는 것이다.

가운데 그림에 이는 두 개의 공식. 위의 공식은 최소 비용을 계산하는 cost 함수,
아래는 기울기(W)의 다음 번 위치를 판단하기 위한 변량(gradients)을 계산하기 위한 공식.

W = W - 변화량

변화량을 구하는 가장 쉽고 일반적인 수학공식은 미분(deravative)이다.
나 또한 미분을 잘 모른다. 스터디 회원의 도움을 받아 작성 중임을 고백한다.
여기서 중요한 것은 기울기(W)를 얼마나, 어떤 방향으로 변화시킬 것인지이다.
현재 위치에서 변화량을 계산해서 현재의 W에서 빼면, 다음 번 W의 위치가 나오게 된다.
앞에서 나왔던 빨간 점이 찍힌 밥그릇 그래프를 보면,
밥그릇의 왼쪽 경사에 있을 때는 변화량이 음수가 되어서 W의 값이 증가하게 되고
밥그릇의 오른쪽 경사에 있을 때는 변화량이 양수가 되어서 W의 값이 감소하게 된다.
결국 어느 위치에 있건 중앙에 있는 cost가 가장 적게 발생하는 최소 비용에 수렴하게 된다.


밥그릇의 오른쪽에 있을 때 기울기가 양수가 된다고 얘기했다.
미분(기울기)은 수평(x축)으로 이동할 때, 수직(y축)으로 이동한 만큼의 크기를 말한다.
밥그릇 오른쪽에서는 x축도 증가하고 y축도 증가하기 때문에 크기의 차이는 있을 수 있지만, 결과는 양수가 된다.
밥그릇 왼쪽에서는 x축이 오른쪽으로 이동(증가)할 때 y축이 아래로 이동(감소)하기 때문에 결과는 음수가 된다.


그렇다면 '변화량'은 어떻게 구할 것인가? 현재 위치에서 발생한 cost에 대해 미분을 하면 된다.
그래프에서 수평은 W, 수직은 Cost라고 강조했는데, 정말 중요하다.
밥그릇에서의 미분은 W가 변하는 크기에 따른 Cost의 변화량을 측정하는 것이다.
그래서, Cost에 대해서 W로 미분을 하게 된다.
학교에서 배운 그래프에서는 x가 변할 때의 y 변화량을 계산하기 때문에 항상 x에 대해 미분을 했었다.

가로와 세로는 W와 b, 높이는 cost를 가리킨다. W와 b의 값에 따른 cost를 표현하고 있다.
cost를 계산했을 때의 결과를 보여주기 때문에, 쉽게 말해 cost 함수라고 얘기할 수 있다.
오른쪽 그림의 윗부분에 cost 함수의 계산식이 나와 있다.

cost 함수는 왼쪽처럼 울퉁불퉁한 형태로 동작하지 않게 만들어야 한다.
반드시 오른쪽처럼 오목한 형태가 되도록 만들어야 하는데,
이를 convex 함수라고 부른다.
gradient descent 알고리듬은 convex(오목, 밥그릇) 형태에 대해서만 적용할 수 있기 때문에 매우 중요하다.


최근에 공부한 내용을 통해 모멘텀(momentum) 등의 알고리듬을 사용하면 어느 정도의 로컬 최저점을 넘어설 수 있다는
것을 알았다.
최저점을 향해 내려갈 때 관성이나 가속도같은 운동량(momentum, 기세)을 줘서 반대편으로 살짝 올라갈 수 있게
처리하는 알고리듬이 모멘텀이다.


'''


# 텐서플로우를 사용해서 비용(cost)과 기울기(W)가 어떻게 변화하는지 보여주는 코드를 설명한다.
import tensorflow as tf

# X와 Y 데이터를 1x3 매트릭스로 초기화했고, m은 데이터의 갯수를 뜻한다.
# 기울기(W)를 바꾸면서 비용을 계산해서 보여주려는 것이 목적이기 때문에 W를 placeholder로 처리했다.
# 여기서는 y 절편에 해당하는 b는 생략하고 있다.
# 다음 번 동영상에서 절편 b를 처리하는 쉬운 방법을 배운다. 여기서는 생략했고,
# 결과를 명확하게 보여주기 위한 것이라고 생각하자.
#
X = [1., 2., 3.]
Y = [1., 2., 3.]
m = len(X)
print('m ==>', m)

W = tf.placeholder(tf.float32)


# H(x) = Wx 로 식을 간단하게 정리했으므로, hypothesis에서도 b를 더하는 코드는 보이지 않는다.
# cost를 계산하는 코드에서는 square 함수 대신 pow 함수를 사용하고 있다.
# square 함수는 제곱을 처리하고, pow 함수는 지수를 처리한다.
# pow 함수이기 때문에 두 번째 매개변수로 2가 전달되었다.
# 평균을 구하기 위해 reduce_mean 함수 대신 m으로 직접 나누고 있다.
# 위의 코드를 이전 코드에서는 아래처럼 처리했었다.
hypothesis = tf.multiply(W, X)
cost = tf.reduce_sum(tf.pow(hypothesis-Y, 2)) / m

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

# 그래프로 표시하기 위해 데이터를 누적할 리스트
W_val, cost_val = [], []


# [출력 결과]는 그래프에 표시된 좌표를 보여준다. -3에서 5까지 81개의 좌표가 나왔다.
# 출력된 숫자는 왼쪽 열이 W, 오른쪽 열이 Cost가 된다.
# 이 코드에서는 최소 비용을 찾는 것이 목적이 아니라 최소 비용을 포함하고 있는 W의 값까지 포함해서
# 일정 범위의 W에 대한 Cost를 계산해서, Cost가 어떻게 변화하는지 보여주는 것이 목적이다.
# 그래프는 W가 변화할 때의 Cost를 보여준다는 것을 명심하자.
# 앞선 글에서 빨강 점 단위로 이동한다고 가정할 때,
# 해당 위치(Cost)에서 미분한 결과를 빼면 가능하다는 것을 설명했었다.
# 그래프에 출력하기 위해서 W_val과 cost_val 변수를 리스트로 만들었다.
# 81번의 호출 결과를 저장해야 하니까.
# 원본 코드에서는 50인데, 나는 51을 사용해서 왼쪽과 오른쪽의 균형을 맞추었다.
# 원본에는 중복되는 코드가 있어서 여기서는 xPos와 yPos로 대체했다. 코드는 몇 줄 길어졌다.
#
# 0.1 단위로 증가할 수 없어서 -30부터 시작. 그래프에는 -3에서 5까지 표시됨.
for i in range(-30, 51):
    xPos = i*0.1                                    # x 좌표. -3에서 5까지 0.1씩 증가
    yPos = sess.run(cost, feed_dict={W: xPos})      # x 좌표에 따른 y 값

    print('{:3.1f}, {:3.1f}'.format(xPos, yPos))

    # 그래프에 표시할 데이터 누적. 단순히 리스트에 갯수를 늘려나감
    W_val.append(xPos)
    cost_val.append(yPos)

sess.close()

# ------------------------------------------ #
# matplotlib 모듈은 파이썬에서 가장 많이 사용하는 그래프 출력 모듈이다.
# 빨강 점이 찍힌  밥그릇 그래프가 여기서 출력됐다.
# plot 함수의 첫 번째는 x축 데이터, 두 번째는 y축 데이터, 세 번째는 그래프 타입.
# r은 빨강의 red, o는 동그라미를 의미한다.
# o 대신 x를 사용하면 x 표시로 바뀐다. 그래프는 show 함수를 호출하기 전까지는 출력되지 않는다.

import matplotlib.pyplot as plt

plt.plot(W_val, cost_val, 'ro')
plt.ylabel('Cost')
plt.xlabel('W')
plt.show()

print('------------------------------------------------------------------------------')
print('placeholder를 사용하는 것과 동시에 앞의 글에서 설명했던, 현재 cost에 대한 미분 결과를 어떻게 계산하는지 보여준다.')
'''
여기서 한발 더 나아가서 텐서플로우가 계산한 값과 직접 계산한 값이 똑같다는 것까지 보여준다.

이번 코드는 동영상과 많은 부분에서 다르다. 
동영상에서는 수동으로 계산해도 최소 비용을 잘 찾는다는 것을 보여주는 반면, 
여기서는 앞에 설명한 내용들을 보여준다.

'''

# import tensorflow as tf

x_data = [1., 2., 3., 4.]
y_data = [1., 3., 5., 7.]   # x와 y의 관계가 모호하다. cost가 내려가지 않는 것이 맞을 수도 있다.

# 동영상에 나온 데이터셋. 40번째 위치에서 best fit을 찾는다. 이번에는 사용하지 않음.
# x_data = [1., 2., 3.]
# y_data = [1., 2., 3.]

W = tf.Variable(tf.random_uniform([1], -10000., 10000.))        # tensor 객체 반환

X = tf.placeholder(tf.float32)      # 반복문에서 x_data, y_data로 치환됨
Y = tf.placeholder(tf.float32)

hypothesis = W * X
cost = tf.reduce_mean(tf.square(hypothesis - Y))


# 이번 코드는 해석이 어렵다. 일단 GradientDescentOptimizer 함수를 호출하지 않고 있다.
# update 계산을 통해 얻은 W를 cost에 전달하고 있다.
# 그럼에도 불구하고 cost는 정상적으로 최저점을 찾아서 진행한다.
# 이 말은 update 계산에 포함된 공식이 올바르게 GradientDescentOptimizer 함수의 역할을 하고 있다는 뜻이 된다.
# 즉, GradientDescentOptimizer 함수 없이 직접 만들어서 구동할 수도 있다는 것을 보여주는 예제이다.
# 동영상에서 미분을 적용해서 구한 새로운 공식. cost를 계산하는 공식
# 코드와 공식을 함께 보자.
#
# W(x)                      -->           tf.mul(W, X)
# W(x) - y                  -->           tf.mul(W, X) - Y
# (W(x) - y) * x            -->           tf.mul(tf.mul(W, X) - Y, X)
# 1/m * ∑(W(x) - y) * x)    -->           tf.reduce_mean(tf.mul(tf.mul(W, X) - Y, X))
# α(1/m * ∑(W(x) - y) * x)  -->           tf.mul(0.01, tf.reduce_mean(tf.mul(tf.mul(W, X) - Y, X)))
#
# 복잡하기는 하지만, 하나씩 순서대로 확장해 가니 조금 쉬워 보인다.
# 혹시 브라우저에 따라 수직으로 줄을 맞춘 부분이 어긋날 수도 있을 것 같다. 맥 크롬에서는 얼추 맞았다.
mean    = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))   # 변경된 W가 mean에도 영향을 준다
descent = W - tf.multiply(0.01, mean)


# W 업데이트. tf.assign(W, descent). 호출할 때마다 변경된 W의 값이 반영되기 때문에 업데이트된다.
update  = W.assign(descent)

init = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init)

for step in range(50):
    uResult = sess.run(update, feed_dict={X: x_data, Y: y_data})    # 이 코드를 호출하지 않으면, W가 바뀌지 않는다.
    cResult = sess.run(  cost, feed_dict={X: x_data, Y: y_data})    # update에서 바꾼 W가 영향을 주기 때문에 같은 값이 나온다.
    wResult = sess.run(W)
    mResult = sess.run(mean, feed_dict={X: x_data, Y: y_data})

    # 결과가 오른쪽과 왼쪽 경사를 번갈아 이동하면서 내려온다. 기존에 한 쪽 경계만 타고 내려오는 것과 차이가 있다.
    # 최종적으로 오른쪽과 왼쪽 경사의 중앙에서 최소 비용을 얻게 된다. (생성된 난수값에 따라 한쪽 경사만 타기도 한다.)
    # descent 계산에서 0.1 대신 0.01을 사용하면 오른쪽 경사만 타고 내려오는 것을 확인할 수 있다. 결국 step이 너무 커서 발생한 현상
    print('{} {} {} [{}, {}]'.format(step, mResult, cResult, wResult, uResult))

print('-'*50)
print('[] 안에 들어간 2개의 결과가 동일하다. 즉, update와 cost 계산값이 동일하다.')

print(sess.run(hypothesis, feed_dict={X: 5.0}))
print(sess.run(hypothesis, feed_dict={X: 2.5}))

sess.close()

